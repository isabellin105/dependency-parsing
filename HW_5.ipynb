{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW_5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tyClcLuqjKb8"
      },
      "source": [
        "# Homework 5: Transition-Based Dependency Parser\n",
        "\n",
        "**Due April 6, 2020 at 11:59pm**\n",
        "\n",
        "In this homework, you will be implementing parts of a transition-based dependency parser."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aJLC8xn_jKcA"
      },
      "source": [
        "**Before beginning, please switch your Colab session to a GPU runtime** \n",
        "\n",
        "Go to Runtime > Change runtime type > Hardware accelerator > GPU\n",
        "\n",
        "## ALSO, REMEMBER TO UPLOAD THE DATASET!\n",
        "\n",
        "Click the Files icon > Upload > Upload `train.projective.short.conll` and `dev.projective.conll` that you have downloaded from bCourses:Files/HW_5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4iICR_E0jKcC"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X1TYSGJVjKcD",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import re\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rNQBqiH3jKcH",
        "outputId": "81714947-a53e-408a-dd2d-f8a8039fbeeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# if this cell prints \"Running on cpu\", you must switch runtime environments\n",
        "# go to Runtime > Change runtime type > Hardware accelerator > GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on {}\".format(device))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o0t2Z1vRjKcL"
      },
      "source": [
        "### Download pretrained word embeddings\n",
        "\n",
        "In this assignment, we will still be using [GloVe](https://nlp.stanford.edu/projects/glove/) pretrained word embeddings.\n",
        "\n",
        "**Note**: this section will take *several minutes*, since the embedding files are large. Files in Colab may be cached between sessions, so you may or may not need to redownload the files each time you reconnect. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0u26MFsgjKcM",
        "outputId": "a07721aa-0f08-46a3-a4ad-6b4a54a136b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-05 18:43:58--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-04-05 18:43:59--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-04-05 18:43:59--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.19MB/s    in 6m 30s  \n",
            "\n",
            "2020-04-05 18:50:29 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "njKMNfHmjKcR"
      },
      "source": [
        "### Question 1. Checking for Projectivity\n",
        "In this question, you are supposed to implement the `is_projective` function below.\n",
        "* A tree structure is said to be [projective](https://en.wikipedia.org/wiki/Discontinuity_(linguistics)) if there are no crossing dependency edges and/or projection lines. \n",
        "* The function should take a sentence as input and returns True if and only if the tree is projective."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fME7S_whjKcT",
        "colab": {}
      },
      "source": [
        "def is_projective(toks):\n",
        "    \"\"\"\n",
        "    params: toks is a list of (idd, tok, pos, head, lab) for a sentence\n",
        "    return True if and only if the sentence has a projective dependency tree\n",
        "    \"\"\"\n",
        "\n",
        "    # Implement your code below\n",
        "    \n",
        "    ##################\n",
        "    graph = {}\n",
        "    for i in range(len(toks)+1):\n",
        "          graph[i] = []\n",
        "    for elem in toks:\n",
        "      graph.setdefault(elem[3],[]).append(elem[0])\n",
        "\n",
        "    visited = []\n",
        "\n",
        "    def dfs(visited, graph, node):\n",
        "      if node not in visited:\n",
        "        visited.append(node)\n",
        "        for neighbour in graph[node]:\n",
        "          dfs(visited, graph, neighbour)\n",
        "    \n",
        "    v = []\n",
        "    for i in range(len(toks)+1):\n",
        "      dfs(visited, graph, i)\n",
        "      v.append(visited)\n",
        "      visited = []\n",
        "\n",
        "    for elem in toks:\n",
        "      dep = elem[0]\n",
        "      head = elem[3]\n",
        "      if (head - dep > 1):\n",
        "        nodes = np.arange(dep+1, head)  \n",
        "        if (np.any([node not in v[head] for node in nodes])):\n",
        "          return False\n",
        "      if (dep - head > 1):\n",
        "        nodes = np.arange(head+1, dep)  \n",
        "        if (np.any([node not in v[head] for node in nodes])):\n",
        "          return False\n",
        "    return True\n",
        "    ##################\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ccPr3IevJFd8",
        "outputId": "3a1d075b-054f-4318-c060-6656391a98bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def sanity_check_is_projective():\n",
        "    \"\"\"\n",
        "    Sanity check for the function is_projective()\n",
        "    \"\"\"\n",
        "    # \"From the AP comes this story:\" should be projective\n",
        "    proj_toks = [(1, 'From', 'IN', 3, 'case'), \n",
        "                  (2, 'the', 'DT', 3, 'det'), \n",
        "                  (3, 'AP', 'NNP', 4, 'obl'), \n",
        "                  (4, 'comes', 'VBZ', 0, 'root'), \n",
        "                  (5, 'this', 'DT', 6, 'det'), \n",
        "                  (6, 'story', 'NN', 4, 'nsubj'), \n",
        "                  (7, ':', ':', 4, 'punct')]\n",
        "    assert is_projective(proj_toks) == True\n",
        "    \n",
        "    # \"I saw a man today who is tall\" should not be projective\n",
        "    non_proj_toks = [(1, 'I', 'PRP', 2, 'nsubj'), \n",
        "                      (2, 'saw', 'VBD', 0, 'root'), \n",
        "                      (3, 'a', 'DT', 4, 'det'), \n",
        "                      (4, \"man\", 'NN', 2, 'obj'), \n",
        "                      (5, 'today', 'NN', 2, 'nmod'), \n",
        "                      (6, 'who', 'WP', 8, 'nsubj'), \n",
        "                      (7, 'is', 'VBZ', 8, 'cop'), \n",
        "                      (8, 'tall', 'JJ', 4, 'acl:relcl')]\n",
        "    assert is_projective(non_proj_toks) == False\n",
        "    print(\"Congrats! You have passed the basic sanity check of is_projective().\")\n",
        "    \n",
        "sanity_check_is_projective()    "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Congrats! You have passed the basic sanity check of is_projective().\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eCZUcO60jKcW"
      },
      "source": [
        "### Question 2.a.\n",
        "Implement the first helper function `perform_shift` to achieve the SHIFT operation.\n",
        "* The SHIFT Operation removes the word from the front of the input buffer and pushes it onto stack."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CFzJksVqjKcX",
        "colab": {}
      },
      "source": [
        "def perform_shift(wbuffer, stack, arcs,\n",
        "                  configurations, gold_transitions):\n",
        "    \"\"\"\n",
        "    perform the SHIFT operation\n",
        "    \"\"\"\n",
        "\n",
        "    # Implement your code below\n",
        "    # your code should:\n",
        "    # 1. append the latest configuration to configurations\n",
        "    # 2. append the latest action to gold_transitions\n",
        "    # 3. update wbuffer, stack and arcs accordingly\n",
        "    # hint: note that the order of operations matters\n",
        "    # as we want to capture the configurations and transition rules\n",
        "    # before making changes to the stack, wbuffer and arcs\n",
        "    \n",
        "    ##################\n",
        "    w = wbuffer[:]\n",
        "    s = stack[:]\n",
        "    a = arcs[:]\n",
        "\n",
        "    configurations.append((w,s,a))\n",
        "    gold_transitions.append('SHIFT')\n",
        "    n = wbuffer.pop()\n",
        "    stack.append(n)\n",
        "    ##################\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nt_K4zkCJRcV",
        "outputId": "27e85029-24fd-42f1-ecf5-e176ab2a8d38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def sanity_check_perform_shift():\n",
        "    \"\"\"\n",
        "    Sanity check for the function perform_shift()\n",
        "    \"\"\"    \n",
        "    # Before perform SHIFT\n",
        "    wbuffer = [3, 2, 1]\n",
        "    stack = [0]\n",
        "    arcs = []\n",
        "    configurations = []\n",
        "    gold_transitions = []\n",
        "\n",
        "    # Perform SHIFT\n",
        "    perform_shift(wbuffer, stack, arcs, configurations, gold_transitions)\n",
        "\n",
        "    # After perform SHIFT\n",
        "    assert wbuffer == [3, 2], \"The result for wbuffer is not correct\"\n",
        "    assert stack == [0, 1], \"The result for stack is not correct\"\n",
        "    assert arcs == [], \"The result for arcs is not correct\"\n",
        "    assert configurations == [([3, 2, 1], [0], [])], \"The result for configurations is not correct\"\n",
        "    assert gold_transitions == ['SHIFT'], \"The result for gold_transitions is not correct\"\n",
        "    print(\"Cool! You have passed the basic sanity check of perform_shift().\")\n",
        "    \n",
        "sanity_check_perform_shift()    "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cool! You have passed the basic sanity check of perform_shift().\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JxqIrNnzjKcb"
      },
      "source": [
        "### Question 2.b.\n",
        "Implement the second helper function `perform_arc` to achieve the ARC operation.\n",
        "\n",
        "* LEFT-ARC (label): assert relation between head at $stack_1$ and dependent at $stack_2$: remove $stack_2$\n",
        "* RIGHT-ARC (label): assert relation between head at $stack_2$ and dependent at $stack_1$; remove $stack_1$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NW9_Y2fyjKcc",
        "colab": {}
      },
      "source": [
        "def perform_arc(direction, dep_label,\n",
        "                wbuffer, stack, arcs,\n",
        "                configurations, gold_transitions):\n",
        "    \"\"\"\n",
        "    params:\n",
        "        - direction: {\"LEFT\", \"RIGHT\"}\n",
        "        - dep_label: label for the dependency relations\n",
        "    Perform LEFTARC_ and RIGHTARC_ operations\n",
        "    \"\"\"\n",
        "\n",
        "    # Implement your code below\n",
        "    # your code should:\n",
        "    # 1. append the latest configuration to configurations\n",
        "    # 2. append the latest action to gold_transitions\n",
        "    # 3. update wbuffer, stack and arcs accordingly\n",
        "    # hint: note that the order of operations matters\n",
        "    # as we want to capture the configurations and transition rules\n",
        "    # before making changes to the stack, wbuffer and arcs\n",
        "\n",
        "    ##################\n",
        "    w = wbuffer[:]\n",
        "    s = stack[:]\n",
        "    a = arcs[:]\n",
        "\n",
        "    configurations.append((w,s,a))\n",
        "    gold_transitions.append(direction+'ARC_'+dep_label)\n",
        "    if direction == 'RIGHT':\n",
        "      n = stack.pop()\n",
        "      arcs.append((dep_label, stack[len(stack)-1],n))\n",
        "    elif direction == 'LEFT':\n",
        "      n = stack.pop(len(stack)-2)\n",
        "      arcs.append((dep_label, stack[len(stack)-1], n))\n",
        "    ##################\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YbvwB7kkJV95",
        "outputId": "f119e059-b86b-4515-ff89-ae32dd0f880d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def sanity_check_perform_arc():\n",
        "    \"\"\"\n",
        "    Sanity check for the function perform_arc()\n",
        "    \"\"\"\n",
        "    # Before perform ARC\n",
        "    direction = 'RIGHT'\n",
        "    dep_label = 'punct'\n",
        "    wbuffer = [5, 4, 3]\n",
        "    stack = [0, 1, 2]\n",
        "    arcs = []\n",
        "    configurations = [([5, 4, 3, 2, 1], [0], []), \n",
        "                      ([5, 4, 3, 2], [0, 1], [])]\n",
        "    gold_transitions = ['SHIFT', 'SHIFT']\n",
        "\n",
        "    # Perform ARC\n",
        "    perform_arc(direction, dep_label, wbuffer, stack, arcs, configurations, gold_transitions)\n",
        "\n",
        "    # After perform ARC\n",
        "    assert wbuffer == [5, 4, 3], \"The result for wbuffer is not correct\"\n",
        "    assert stack == [0, 1], \"The result for stack is not correct\"\n",
        "    assert arcs == [('punct', 1, 2)], \"The result for arcs is not correct\"\n",
        "    assert configurations == [([5, 4, 3, 2, 1], [0], []), \n",
        "                              ([5, 4, 3, 2], [0, 1], []), \n",
        "                              ([5, 4, 3], [0, 1, 2], [])], \\\n",
        "            \"The result for configurations is not correct\"\n",
        "    assert gold_transitions == ['SHIFT', 'SHIFT', 'RIGHTARC_punct'], \"The result for gold_transitions is not correct\"\n",
        "    print(\"You have passed the basic sanity check of perform_arc().\")\n",
        "\n",
        "sanity_check_perform_arc()    "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You have passed the basic sanity check of perform_arc().\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mmDSyQpqjKcg"
      },
      "source": [
        "### Question 2.c.\n",
        "Now, since we have implemented the helper functions, let's use them to complete `tree_to_actions`.\n",
        "\n",
        "`tree_to_actions` takes wbuffer, stack, arcs and deps as input, returns configuration of the parser and action for the parser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9HeTVArDjKci",
        "colab": {}
      },
      "source": [
        "def tree_to_actions(wbuffer, stack, arcs, deps):\n",
        "    \"\"\"\n",
        "    params:\n",
        "    wbuffer: a list of word indices; the top of buffer is at the end of the list\n",
        "    stack: a list of word indices; the top of buffer is at the end of the list\n",
        "    arcs: a list of (label, head, dependent) tuples\n",
        "\n",
        "    Given wbuffer, stack, arcs and deps\n",
        "    Return configurations and gold_transitions (actions)\n",
        "    \"\"\"\n",
        "\n",
        "    # configurations:\n",
        "    # A list of tuples of lists\n",
        "    # [(wbuffer1, stack1, arcs1), (wbuffer2, stack2, arcs2), ...]\n",
        "    # Keeps tracks of the states at each step\n",
        "    configurations=[]\n",
        "\n",
        "    # gold_transitions:\n",
        "    # A list of action strings, e.g [\"SHIFT\", \"LEFTARC_nsubj\"]\n",
        "    # Keeps tracks of the actions at each step\n",
        "    gold_transitions=[]\n",
        "\n",
        "    # Implement your code below\n",
        "    # hint:\n",
        "    # 1. configurations[i] and gold_transitions[i] should\n",
        "    # correspond to the states of the wbuffer, stack, arcs\n",
        "    # (before the action was taken) and action to take at step i\n",
        "    # 2. you should call perform_shift and perform_arc in your code\n",
        "    \n",
        "\n",
        "    ##################\n",
        "    configurations.append((wbuffer,stack,arcs))\n",
        "    gold_transitions.append('SHIFT')\n",
        "    perform_shift(wbuffer, stack, arcs,configurations, gold_transitions)\n",
        "\n",
        "    while (len(stack)>1 or len(wbuffer)!=0):\n",
        "      stack_1 = stack[len(stack)-1]\n",
        "      stack_2 = stack[len(stack)-2]\n",
        "      if (len(stack)>1 and stack_1 in deps.keys() and (stack_1,stack_2) in deps[stack_1]):\n",
        "        direction = 'LEFT'\n",
        "        dep_label = deps[stack_1][(stack_1,stack_2)]\n",
        "        configurations.append((wbuffer,stack,arcs))\n",
        "        gold_transitions.append(direction+'ARC_'+dep_label)\n",
        "        perform_arc(direction, dep_label, wbuffer, stack, arcs, configurations, gold_transitions)\n",
        "      elif (len(stack)>1 and stack_2 in deps.keys() and (stack_2,stack_1) in deps[stack_2] and all([(stack_1, w) not in deps.get(stack_1, []) for w in wbuffer])):\n",
        "        direction = 'RIGHT'\n",
        "        dep_label = deps[stack_2][(stack_2,stack_1)]\n",
        "        configurations.append((wbuffer,stack,arcs))\n",
        "        gold_transitions.append(direction+'ARC_'+dep_label)\n",
        "        perform_arc(direction, dep_label, wbuffer, stack, arcs, configurations, gold_transitions)\n",
        "      elif (len(wbuffer)!=0):\n",
        "        configurations.append((wbuffer,stack,arcs))\n",
        "        gold_transitions.append('SHIFT')\n",
        "        perform_shift(wbuffer, stack, arcs,configurations, gold_transitions)\n",
        "    return configurations, gold_transitions\n",
        "    ##################\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fYpLgArZJb1X",
        "outputId": "34c9b875-441c-43a6-e4f2-9fba774828db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def sanity_check_tree_to_actions():\n",
        "    \"\"\"\n",
        "    Sanity check for the function tree_to_actions()\n",
        "    \"\"\"\n",
        "    # Before tree_to_actions \n",
        "    wbuffer = [9, 8, 7, 6, 5, 4, 3, 2, 1]\n",
        "    stack = [0]\n",
        "    arcs = []\n",
        "    deps = {5: {(5, 9): 'punct', (5, 8): 'obl', (5, 4): 'advmod', (5, 3): 'aux:pass', (5, 2): 'nsubj:pass'},\n",
        "            8: {(8, 7): 'det', (8, 6): 'case'}, 0: {(0, 5): 'root'}, 2: {(2, 1): 'nmod:poss'}}\n",
        "    \n",
        "    tree_to_actions(wbuffer, stack, arcs, deps)\n",
        "    # After tree_to_actions\n",
        "    assert wbuffer == [], \"The result for wbuffer is not correct\"\n",
        "    assert stack == [0], \"The result for stack is not correct\"\n",
        "    assert arcs == [('nmod:poss', 2, 1), ('advmod', 5, 4), ('aux:pass', 5, 3), ('nsubj:pass', 5, 2), \n",
        "                     ('det', 8, 7), ('case', 8, 6), ('obl', 5, 8), ('punct', 5, 9), ('root', 0, 5)], \\\n",
        "                        \"The result for arcs is not correct\"\n",
        "    assert deps == {5: {(5, 9): 'punct', (5, 8): 'obl', (5, 4): 'advmod', (5, 3): 'aux:pass', (5, 2): 'nsubj:pass'}, \n",
        "                     8: {(8, 7): 'det', (8, 6): 'case'}, 0: {(0, 5): 'root'}, 2: {(2, 1): 'nmod:poss'}}, \\\n",
        "                    \"The result for deps is not correct\"\n",
        "    print(\"You have passed the basic sanity check of tree_to_actions()! One more function to go.\")   \n",
        "     \n",
        "sanity_check_tree_to_actions()    "
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You have passed the basic sanity check of tree_to_actions()! One more function to go.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IlLh-Q_sjKcl"
      },
      "source": [
        "### Question 3. Tree Parsing with Predictions\n",
        "Implement action_to_tree, which will update the dependency tree based on the action predictions.\n",
        "* Don't forget to use `isvalid` to check the validity of the possible actions!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pFTfAwLWjKcm",
        "colab": {}
      },
      "source": [
        "def isvalid(stack, wbuffer, action):\n",
        "    \"\"\"\n",
        "    Helper function that returns True only if an action is\n",
        "    legal given the current states of the stack and wbuffer\n",
        "    \"\"\"\n",
        "    if action == \"SHIFT\" and len(wbuffer) > 0:\n",
        "        return True\n",
        "    if action.startswith(\"RIGHTARC\") and len(stack) > 1 and stack[-1] != 0:\n",
        "        return True\n",
        "    if action.startswith(\"LEFTARC\") and len(stack) > 1 and stack[-2] != 0:\n",
        "        return True\n",
        "\n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RH_vzMzRjKcp",
        "colab": {}
      },
      "source": [
        "def action_to_tree(tree, predictions, wbuffer, stack, arcs, reverse_labels):\n",
        "    \"\"\"\n",
        "    params:\n",
        "    tree:\n",
        "    a dictionary of dependency relations (head, dep_label)\n",
        "        {\n",
        "            child1: (head1, dep_lebel1),\n",
        "            child2: (head2, dep_label2), ...\n",
        "        }\n",
        "\n",
        "    predictions:\n",
        "    a numpy column vector of probabilities for different dependency labels\n",
        "    as ordered by the variable reverse_labels\n",
        "    predictions.shape = (1, total number of dependency labels)\n",
        "\n",
        "    wbuffer: a list of word indices; top of buffer is at the end of the list\n",
        "    stack: a list of word indices; top of stack is at the end of the list\n",
        "    arcs: a list of (label, head, dependent) tuples\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Implement your code below\n",
        "    # hint:\n",
        "    # 1. the predictions contains the probability distribution for all\n",
        "    # possible actions for a single step, and you should choose one\n",
        "    # and update the tree only once\n",
        "    # 2. some actions predicted are not going to be valid\n",
        "    # (e.g., shifting if nothing is on the buffer)\n",
        "    # so sort probs and keep going until you find one that is valid.\n",
        "    \n",
        "    ##################\n",
        "    idxs = (-predictions).argsort()[:3]\n",
        "    for idx in idxs[0]:\n",
        "      if (isvalid(stack, wbuffer, reverse_labels[idx])):\n",
        "        if (reverse_labels[idx] == \"SHIFT\"):\n",
        "          n = wbuffer.pop()\n",
        "          stack.append(n)\n",
        "          break\n",
        "        if (reverse_labels[idx].startswith(\"RIGHTARC\")):\n",
        "          n = stack.pop()\n",
        "          arcs.append((reverse_labels[idx][9:], stack[len(stack)-1],n))\n",
        "          tree[n] = (stack[len(stack)-1],reverse_labels[idx][9:])\n",
        "          break\n",
        "        elif (reverse_labels[idx].startswith(\"LEFTARC\")):\n",
        "          n = stack.pop(len(stack)-2)\n",
        "          arcs.append((reverse_labels[idx][8:], stack[len(stack)-1], n))\n",
        "          tree[n] = (stack[len(stack)-1],reverse_labels[idx][8:])\n",
        "          break\n",
        "    ##################\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W7ZoAad9JiPF",
        "outputId": "06c2dcd2-d0e3-4f53-8093-9edbf09d684c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def sanity_check_action_to_tree():\n",
        "    \"\"\"\n",
        "    Sanity check for the function action_to_tree()\n",
        "    \"\"\"    \n",
        "    # Before action\n",
        "    tree = {}\n",
        "    predictions = np.array([[ 8.904456  ,  2.1306312 , -0.6716528 , -0.37662476, -0.01239625,-3.3660867 , -2.1345713 ,  1.4581618 , \n",
        "                             -0.1688145 , -0.61321   , 0.40860286, -2.7569351 , -0.69548404, -0.7809651 ,  0.7595304 ,-2.770731  , \n",
        "                             -0.97373027, -2.70085   , -0.26645675, -1.2353135 ,-1.4289687 , -1.3272284 , -2.4956157 , -1.0178847 , \n",
        "                             -1.7484616 , 1.7610879 ,  0.301237  , -0.71727145, -1.9370077 , -1.3722429 , 0.9516849 , -2.6749346 , \n",
        "                             -1.4604743 , -1.6903474 , -2.5261753 ,-0.88417345, -0.50328434, -0.21296862, -3.4296887 , -3.3282495 ,\n",
        "                             -4.300956  , -2.12365   , -3.3637137 , -5.570282  , -3.8983932 ,-3.0985348 , -5.818429  , -1.5155774 , \n",
        "                             -3.4247532 , -2.7098398 ,-4.799152  , -4.020282  , -3.5505116 , -2.7114115 , -4.1488724 ,-4.7484784 , \n",
        "                             -4.0955606 , -2.994336  , -4.9744525 , -4.3390574 ,-2.782462  , -4.615161  , -4.6250424 , -4.4105268 , \n",
        "                             -4.856515  ,-3.5684056 , -4.6808653 , -4.882898  , -4.3673973 , -5.379696  ]])\n",
        "    \n",
        "    reverse_labels = ['SHIFT', 'RIGHTARC_punct', 'RIGHTARC_flat', 'LEFTARC_amod', 'LEFTARC_nsubj', 'LEFTARC_det', 'RIGHTARC_appos', 'RIGHTARC_obj', 'LEFTARC_case', 'RIGHTARC_nmod', 'RIGHTARC_obl', 'RIGHTARC_parataxis', 'RIGHTARC_root', 'LEFTARC_aux', 'LEFTARC_punct', 'RIGHTARC_iobj', 'LEFTARC_mark', 'RIGHTARC_acl', 'RIGHTARC_compound:prt', 'LEFTARC_nummod', 'RIGHTARC_ccomp', 'LEFTARC_aux:pass', 'LEFTARC_nsubj:pass', 'LEFTARC_compound', 'LEFTARC_nmod:poss', 'LEFTARC_cc', 'RIGHTARC_conj', 'LEFTARC_advmod', 'RIGHTARC_xcomp', 'LEFTARC_advcl', 'RIGHTARC_advmod', 'RIGHTARC_acl:relcl', 'RIGHTARC_advcl', 'LEFTARC_expl', 'RIGHTARC_nsubj', 'LEFTARC_obl', 'LEFTARC_cop', 'RIGHTARC_fixed', 'RIGHTARC_nummod', 'LEFTARC_det:predet', 'RIGHTARC_obl:npmod', 'RIGHTARC_obl:tmod', 'LEFTARC_obl:tmod', 'RIGHTARC_nmod:tmod', 'RIGHTARC_amod', 'LEFTARC_csubj', 'LEFTARC_csubj:pass', 'RIGHTARC_case', 'RIGHTARC_det', 'LEFTARC_obj', 'LEFTARC_nmod:tmod', 'LEFTARC_nmod', 'RIGHTARC_cop', 'RIGHTARC_expl', 'RIGHTARC_aux', 'RIGHTARC_vocative', 'RIGHTARC_csubj', 'LEFTARC_obl:npmod', 'RIGHTARC_nmod:npmod', 'RIGHTARC_list', 'LEFTARC_ccomp', 'LEFTARC_discourse', 'LEFTARC_parataxis', 'LEFTARC_xcomp', 'RIGHTARC_csubj:pass', 'LEFTARC_cc:preconj', 'RIGHTARC_flat:foreign', 'RIGHTARC_compound', 'LEFTARC_acl:relcl', 'RIGHTARC_discourse']\n",
        "    wbuffer = [4,3,2,1]\n",
        "    stack = [0]\n",
        "    arcs = []\n",
        "\n",
        "    # Perform action\n",
        "    action_to_tree(tree, predictions, wbuffer, stack, arcs, reverse_labels)\n",
        "\n",
        "    # After action (the action is SHIFT for this step)\n",
        "    assert not tree, \"The tree should be {} after the SHIFT\"\n",
        "    assert wbuffer == [4,3,2], \"wbuffer should be [4,3,2] after the SHIFT\"\n",
        "    assert stack == [0, 1], \"stack should be [0, 1] after the SHIFT\"\n",
        "    assert arcs == [], \"arcs should be [] after the SHIFT\"\n",
        "    print(\"You have passed the basic sanity check of action_to_tree()!\")\n",
        "    \n",
        "sanity_check_action_to_tree()    "
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You have passed the basic sanity check of action_to_tree()!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pJsWiPY5jKcu"
      },
      "source": [
        "### Implemented for you\n",
        "Now since you have the configuration $x$ and action $y$, we can now train a supervised model to predict an action $y$ given a configuration $x$. We are using a simplified version model of [A Fast and Accurate Dependency Parser using Neural Networks](https://nlp.stanford.edu/pubs/emnlp2014-depparser.pdf).\n",
        "\n",
        "* This model is alreadly implemented for you, please `train` the model, and report the evaluation and test results by calling the function `evaluate` and `test`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vlx_IRrJjKcv",
        "colab": {}
      },
      "source": [
        "# ============================================================\n",
        "# THE FOLLOWING CODE IS PROVIDED\n",
        "# ============================================================\n",
        "def get_oracle(toks):\n",
        "    \"\"\"\n",
        "    Return pairs of configurations + gold transitions (actions)\n",
        "    from training data\n",
        "    configuration = a list of tuple of:\n",
        "        - buffer (top of buffer is at the end of the list)\n",
        "        - stack (top of buffer is at the end of the list)\n",
        "        - arcs (a list of (label, head, dependent) tuples)\n",
        "    gold transitions = a list of actions, e.g. SHIFT\n",
        "    \"\"\"\n",
        "\n",
        "    stack = [] # stack\n",
        "    arcs = [] # existing list of arcs\n",
        "    wbuffer = [] # input buffer\n",
        "\n",
        "    # deps is a dictionary of head: dependency relations, where\n",
        "    # dependency relations is a dictionary of the (head, child): label\n",
        "    # deps = {head1:{\n",
        "    #               (head1, child1):dependency_label1,\n",
        "    #               (head1, child2):dependency_label2\n",
        "    #              }\n",
        "    #         head2:{\n",
        "    #               (head2, child3):dependency_label3,\n",
        "    #               (head2, child4):dependency_label4\n",
        "    #              }\n",
        "    #         }\n",
        "    deps = {}\n",
        "\n",
        "    # ROOT\n",
        "    stack.append(0)\n",
        "\n",
        "    # initialize variables\n",
        "    for position in reversed(toks):\n",
        "        (idd, _, _, head, lab) = position\n",
        "\n",
        "        dep = (head, idd)\n",
        "        if head not in deps:\n",
        "            deps[head] = {}\n",
        "        deps[head][dep] = lab\n",
        "\n",
        "        wbuffer.append(idd)\n",
        "\n",
        "    # configurations:\n",
        "    # A list of (wbuffer, stack, arcs)\n",
        "    # Keeps tracks of the states at each step\n",
        "    # gold_transitions:\n",
        "    # A list of action strings [\"SHIFT\", \"LEFTARC_nsubj\"]\n",
        "    # Keeps tracks of the actions at each step\n",
        "    configurations, gold_transitions = tree_to_actions(wbuffer, stack, arcs, deps)\n",
        "    return configurations, gold_transitions\n",
        "\n",
        "def featurize_configuration(configuration, tokens, postags, vocab, pos_vocab):\n",
        "\n",
        "    def get_id(word, vocab):\n",
        "        word=word.lower()\n",
        "        if word in vocab:\n",
        "            return vocab[word]\n",
        "        return vocab[\"<unk>\"]\n",
        "\n",
        "    \"\"\"\n",
        "    Given configurations of the stack, input buffer and arcs,\n",
        "    words of the sentence and POS tags of the words,\n",
        "    return some features\n",
        "\n",
        "    The current features are the word ID and postag ID at the \n",
        "    first three positions of the stack and buffer.\n",
        "    \"\"\"\n",
        "\n",
        "    wbuffer, stack, arcs = configuration\n",
        "\n",
        "    word_features=[]\n",
        "    pos_features=[]\n",
        "\n",
        "    if len(stack) > 0: \n",
        "        word_features.append(get_id(tokens[stack[-1]], vocab))\n",
        "        pos_features.append(get_id(postags[stack[-1]], pos_vocab))\n",
        "    else: \n",
        "        word_features.append(get_id(\"<NONE>\", vocab))\n",
        "        pos_features.append(get_id(\"<NONE>\", pos_vocab))\n",
        "\n",
        "    if len(stack) > 1: \n",
        "        word_features.append(get_id(tokens[stack[-2]], vocab))\n",
        "        pos_features.append(get_id(postags[stack[-2]], pos_vocab))\n",
        "    else: \n",
        "        word_features.append(get_id(\"<NONE>\", vocab))\n",
        "        pos_features.append(get_id(\"<NONE>\", pos_vocab))\n",
        "\n",
        "    if len(stack) > 2: \n",
        "        word_features.append(get_id(tokens[stack[-3]], vocab))\n",
        "        pos_features.append(get_id(postags[stack[-3]], pos_vocab))\n",
        "    else: \n",
        "        word_features.append(get_id(\"<NONE>\", vocab))\n",
        "        pos_features.append(get_id(\"<NONE>\", pos_vocab))\n",
        "\n",
        "    if len(wbuffer) > 0: \n",
        "        word_features.append(get_id(tokens[wbuffer[-1]], vocab))\n",
        "        pos_features.append(get_id(postags[wbuffer[-1]], pos_vocab))\n",
        "    else: \n",
        "        word_features.append(get_id(\"<NONE>\", vocab))\n",
        "        pos_features.append(get_id(\"<NONE>\", pos_vocab))\n",
        "       \n",
        "    if len(wbuffer) > 1: \n",
        "        word_features.append(get_id(tokens[wbuffer[-2]], vocab))\n",
        "        pos_features.append(get_id(postags[wbuffer[-2]], pos_vocab))\n",
        "    else: \n",
        "        word_features.append(get_id(\"<NONE>\", vocab))\n",
        "        pos_features.append(get_id(\"<NONE>\", pos_vocab))\n",
        "\n",
        "    if len(wbuffer) > 2: \n",
        "        word_features.append(get_id(tokens[wbuffer[-3]], vocab))\n",
        "        pos_features.append(get_id(postags[wbuffer[-3]], pos_vocab))\n",
        "    else: \n",
        "        word_features.append(get_id(\"<NONE>\", vocab))\n",
        "        pos_features.append(get_id(\"<NONE>\", pos_vocab))\n",
        "\n",
        "    return word_features, pos_features\n",
        "\n",
        "\n",
        "def get_oracles(filename, vocab, tag_vocab):\n",
        "    \"\"\"\n",
        "    Get configurations, gold_transitions from all sentences\n",
        "    \"\"\"\n",
        "    with open(filename) as f:\n",
        "        toks, tokens, postags = [], {}, {}\n",
        "        tokens[0] = \"<ROOT>\"\n",
        "        postags[0] = \"<ROOT>\"\n",
        "\n",
        "        # a list of all features for each transition step\n",
        "        word_feats = []\n",
        "        pos_feats = []\n",
        "        # a list of labels, e.g. SHIFT, LEFTARC_DEP_LABEL, RIGHTARC_DEP_LABEL\n",
        "        labels = []\n",
        "\n",
        "        for line in f:\n",
        "            cols = line.rstrip().split(\"\\t\")\n",
        "            \n",
        "            if len(cols) < 2: # at the end of each sentence\n",
        "                if len(toks) > 0:\n",
        "                    if is_projective(toks): # only use projective trees\n",
        "                        # get all configurations and gold standard transitions\n",
        "                        configurations, gold_transitions = get_oracle(toks)\n",
        "                        \n",
        "                        for i in range(len(configurations)):\n",
        "                            word_feat, pos_feat = featurize_configuration(configurations[i], tokens, postags, vocab, tag_vocab)\n",
        "                            label = gold_transitions[i]\n",
        "                            word_feats.append(word_feat)\n",
        "                            pos_feats.append(pos_feat)\n",
        "                            labels.append(label)\n",
        "\n",
        "                    # reset vars for the next sentence\n",
        "                    toks, tokens, postags = [], {}, {}\n",
        "                    tokens[0] = \"<ROOT>\"\n",
        "                    postags[0] = \"<ROOT>\"\n",
        "                    \n",
        "                continue\n",
        "\n",
        "            if cols[0].startswith(\"#\"):\n",
        "                continue\n",
        "\n",
        "            # construct the tuple for each word in the sentence\n",
        "            # for each word in the sentence\n",
        "            # idd: index of a word in a sentence, starting from 1\n",
        "            # tok: the word itself\n",
        "            # pos: pos tag for that word\n",
        "            # head: parent of the dependency\n",
        "            # lab: dependency relation label\n",
        "            idd, tok, pos, head, lab = int(cols[0]), cols[1], cols[4], int(cols[6]), cols[7]\n",
        "            toks.append((idd, tok, pos, head, lab))\n",
        "\n",
        "            # feature for training to predict the gold transition\n",
        "            tokens[idd], postags[idd] = tok, pos\n",
        "\n",
        "        return word_feats, pos_feats, labels\n",
        "\n",
        "def load_embeddings(filename):\n",
        "    # 0 idx is for padding\n",
        "    # 1 idx is for <UNK>\n",
        "    # 2 idx is for <NONE>\n",
        "    # 3 idx is for <ROOT>\n",
        "\n",
        "    # get the embedding size from the first embedding\n",
        "    vocab_size=4\n",
        "    with open(filename, encoding=\"utf-8\") as file:\n",
        "        for idx, line in enumerate(file):\n",
        "            if idx == 0:\n",
        "                word_embedding_dim=len(line.rstrip().split(\" \"))-1\n",
        "            vocab_size+=1\n",
        "        \n",
        "\n",
        "    vocab={\"<pad>\":0, \"<unk>\":1, \"<none>\":2, \"<root>\":3}\n",
        "    print(\"word_embedding_dim: %s, vocab size: %s\" % (word_embedding_dim, vocab_size))\n",
        "\n",
        "    embeddings=np.zeros((vocab_size, word_embedding_dim))\n",
        "\n",
        "    with open(filename, encoding=\"utf-8\") as file:\n",
        "        for idx,line in enumerate(file):\n",
        "\n",
        "            if idx + 4 >= vocab_size:\n",
        "                break\n",
        "\n",
        "            cols=line.rstrip().split(\" \")\n",
        "            val=np.array(cols[1:])\n",
        "            word=cols[0]\n",
        "            embeddings[idx+4]=val\n",
        "            vocab[word]=idx+4\n",
        "\n",
        "    return torch.FloatTensor(embeddings), vocab\n",
        "\n",
        "class ShiftReduceParser(nn.Module):\n",
        "\n",
        "    def __init__(self, embeddings, hidden_dim, tagset_size, num_pos_tags, pos_embedding_dim):\n",
        "        super(ShiftReduceParser, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_labels=tagset_size\n",
        "\n",
        "        _, embedding_dim = embeddings.shape\n",
        "\n",
        "        self.input_size=embedding_dim*6 + pos_embedding_dim*6\n",
        "        \n",
        "        self.dropout_layer = nn.Dropout(p=0.25)\n",
        "\n",
        "        self.word_embeddings = nn.Embedding.from_pretrained(embeddings)\n",
        "        self.pos_embeddings = nn.Embedding(num_pos_tags, pos_embedding_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.W1 = nn.Linear(self.input_size, self.hidden_dim)\n",
        "        self.W2 = nn.Linear(self.hidden_dim, self.num_labels)\n",
        "\n",
        "    def forward(self, words, pos_tags, Y=None):\n",
        "        \n",
        "        words=words.to(device)\n",
        "        pos_tags=pos_tags.to(device)\n",
        "\n",
        "        if Y is not None:\n",
        "            Y=Y.to(device)\n",
        "\n",
        "        word_embeds = self.word_embeddings(words)\n",
        "        postag_embeds = self.pos_embeddings(pos_tags)\n",
        "\n",
        "        embeds=torch.cat((word_embeds, postag_embeds), 2)\n",
        "\n",
        "        embeds=embeds.view(-1, self.input_size)\n",
        "\n",
        "        embeds=self.dropout_layer(embeds)\n",
        "\n",
        "        hidden = self.W1(embeds)\n",
        "        hidden = self.tanh(hidden)\n",
        "        logits = self.W2(hidden)\n",
        "\n",
        "        if Y is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), Y.view(-1))\n",
        "            return loss\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "def get_batches(W, P, Y, batch_size):\n",
        "    batch_W=[]\n",
        "    batch_P=[]\n",
        "    batch_Y=[]\n",
        "\n",
        "    i=0\n",
        "    while i < len(W):\n",
        "        batch_W.append(torch.LongTensor(W[i:i+batch_size]))\n",
        "        batch_P.append(torch.LongTensor(P[i:i+batch_size]))\n",
        "        batch_Y.append(torch.LongTensor(Y[i:i+batch_size]))\n",
        "        i+=batch_size  \n",
        "\n",
        "    return batch_W, batch_P, batch_Y\n",
        "\n",
        "def train(word_feats, pos_feats, labels, embeddings, vocab, postag_vocab, label_vocab):\n",
        "    \"\"\"\n",
        "\n",
        "    Train transition-based parser to predict next action (labels)\n",
        "    given current configuration (featurized by word_feats and pos_feats)\n",
        "    Return the classifier trained using Chen and Manning (2014), \"A Fast \n",
        "    and Accurate Dependency Parser using Neural Networks\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # dimensionality of linear layer\n",
        "    HIDDEN_DIM=100\n",
        "    # dimensionality of POS embeddings\n",
        "    POS_EMBEDDING_SIZE=50\n",
        "\n",
        "    # batch size for training\n",
        "    BATCH_SIZE=32\n",
        "\n",
        "    # number of epochs to train for\n",
        "    NUM_EPOCHS=10\n",
        "\n",
        "    # learning rate for Adam optimizer\n",
        "    LEARNING_RATE=0.001\n",
        "\n",
        "    num_labels=[]\n",
        "    for i, y in enumerate(labels):\n",
        "        num_labels.append(label_vocab[y])\n",
        "\n",
        "    batch_W, batch_P, batch_Y = get_batches(word_feats, pos_feats, num_labels, BATCH_SIZE)\n",
        "\n",
        "    model = ShiftReduceParser(embeddings, HIDDEN_DIM, len(label_vocab), len(postag_vocab), POS_EMBEDDING_SIZE)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model.train()\n",
        "\n",
        "        bigloss=0.\n",
        "        for b in range(len(batch_W)):\n",
        "            model.zero_grad()\n",
        "\n",
        "            loss = model.forward(batch_W[b], batch_P[b], Y=batch_Y[b])\n",
        "            bigloss+=loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"loss: \", bigloss)\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def parse(toks, model, vocab, tag_vocab, reverse_labels):\n",
        "    \"\"\"\n",
        "    parse sentence with trained model and return correctness measure\n",
        "    \"\"\"\n",
        "    tokens, postags = {}, {}\n",
        "    tokens[0] = \"<ROOT>\"\n",
        "    postags[0] = \"<ROOT>\"\n",
        "\n",
        "    wbuffer, stack, arcs = [], [], []\n",
        "    stack.append(0)\n",
        "\n",
        "    for position in reversed(toks):\n",
        "\n",
        "        (idd, tok, pos, head, lab) = position\n",
        "        tokens[idd] = tok\n",
        "        postags[idd] = pos\n",
        "\n",
        "        # update buffer\n",
        "        wbuffer.append(idd)\n",
        "\n",
        "    tree = {}\n",
        "    while len(wbuffer) >= 0:\n",
        "        if len(wbuffer) == 0 and len(stack) == 0: break\n",
        "        if len(wbuffer) == 0 and len(stack) == 1 and stack[0] == 0: break\n",
        "\n",
        "        word_feats, pos_feats = (featurize_configuration((wbuffer, stack, arcs), tokens, postags, vocab, tag_vocab))\n",
        "\n",
        "       \n",
        "        predictions=model.forward(torch.LongTensor([word_feats]), torch.LongTensor([pos_feats]))\n",
        "\n",
        "        predictions=predictions.detach().cpu().numpy()\n",
        "\n",
        "        # your function will be called here\n",
        "        action_to_tree(tree, predictions, wbuffer, stack, arcs, reverse_labels)\n",
        "\n",
        "    return tree\n",
        "\n",
        "def parse_and_evaluate(toks, model, vocab, tag_vocab, reverse_labels):\n",
        "    \"\"\"\n",
        "    parse sentence with trained model and return correctness measure\n",
        "    \"\"\"\n",
        "\n",
        "    heads, labels = {}, {}\n",
        "\n",
        "    for position in reversed(toks):\n",
        "        (idd, tok, pos, head, lab) = position\n",
        "\n",
        "        # keep track of gold standards for performance evaluation\n",
        "        heads[idd], labels[idd] = head, lab\n",
        "\n",
        "    tree = parse(toks, model, vocab, tag_vocab, reverse_labels)\n",
        "\n",
        "    # correct_unlabeled: total number of correct (head, child) dependencies\n",
        "    # correct_labeled: total number of correctly *labeled* dependencies\n",
        "    correct_unlabeled, correct_labeled, total = 0, 0, 0\n",
        "\n",
        "    for child in tree:\n",
        "        (head, label) = tree[child]\n",
        "        if head == heads[child]:\n",
        "            correct_unlabeled += 1\n",
        "            if label == labels[child]: correct_labeled += 1\n",
        "        total += 1\n",
        "\n",
        "    return [correct_unlabeled, correct_labeled, total]\n",
        "\n",
        "def get_label_vocab(labels):\n",
        "    tag_vocab={}\n",
        "    num_labels=[]\n",
        "    for i, y in enumerate(labels):\n",
        "        if y not in tag_vocab:\n",
        "            tag_vocab[y]=len(tag_vocab)\n",
        "        num_labels.append(tag_vocab[y])\n",
        "\n",
        "    reverse_labels=[None]*len(tag_vocab)\n",
        "    for y in tag_vocab:\n",
        "        reverse_labels[tag_vocab[y]]=y\n",
        "\n",
        "    return tag_vocab, reverse_labels\n",
        "\n",
        "\n",
        "def get_pos_tag_vocab(filename):\n",
        "    tag_vocab={\"<none>\":0, \"<unk>\":1}\n",
        "    with open(filename) as file:\n",
        "        for line in file:\n",
        "            cols=line.rstrip().split(\"\\t\")\n",
        "            if len(cols) < 3:\n",
        "                continue\n",
        "            pos=cols[4].lower()\n",
        "            if pos not in tag_vocab:\n",
        "                tag_vocab[pos]=len(tag_vocab)\n",
        "    return tag_vocab\n",
        "\n",
        "def test(model, vocab, tag_vocab, reverse_labels):\n",
        "    \"\"\"\n",
        "    Evaluate the performance of a parser against gold standard\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    toks=[\"I\", \"bought\", \"a\", \"book\"]\n",
        "    pos=[\"NNP\", \"VBD\", \"DT\", \"NN\"]\n",
        "\n",
        "    data=[]\n",
        "    # put it in format parser expects\n",
        "    for i, tok in enumerate(toks):\n",
        "        data.append((i+1, tok, pos[i], \"_\", \"_\"))\n",
        "\n",
        "    tree=parse(data, model, vocab, tag_vocab, reverse_labels)\n",
        "\n",
        "    for child in sorted(tree.keys()):\n",
        "        (head, label) = tree[child]\n",
        "        headStr=\"<ROOT>\"\n",
        "        if head > 0: # child and head indexes start at 1; 0 denotes the <ROOT>\n",
        "            headStr=toks[head-1]\n",
        "\n",
        "        print(\"(%s %s) -> (%s %s) %s\" % (child, toks[child-1], head, headStr, label))\n",
        "  \n",
        "\n",
        "\n",
        "def evaluate(filename, model, vocab, tag_vocab, reverse_labels):\n",
        "    \"\"\"\n",
        "    Evaluate the performance of a parser against gold standard\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with open(filename) as f:\n",
        "        toks=[]\n",
        "        totals = np.zeros(3)\n",
        "        for line in f:\n",
        "            cols=line.rstrip().split(\"\\t\")\n",
        "\n",
        "            if len(cols) < 2: # end of a sentence\n",
        "                if len(toks) > 0:\n",
        "                    if is_projective(toks):\n",
        "                        tots = np.array(parse_and_evaluate(toks, model, vocab, tag_vocab, reverse_labels))\n",
        "                        totals += tots\n",
        "                        \n",
        "                    toks = []\n",
        "                continue\n",
        "\n",
        "            if cols[0].startswith(\"#\"):\n",
        "                continue\n",
        "\n",
        "            idd, tok, pos, head, lab = int(cols[0]), cols[1], cols[4], int(cols[6]), cols[7]\n",
        "            toks.append((idd, tok, pos, head, lab))\n",
        "        \n",
        "        print (\"UAS: %.3f, LAS:%.3f\" % (totals[0]/totals[2], totals[1]/totals[2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ax1VrekKjKcy"
      },
      "source": [
        "### Train and evaluate the model\n",
        "\n",
        "- NOTICE: Because you are not implementing the model or the training process, You will **NOT** be graded based on the performance of the model!\n",
        "\n",
        "- You are only graded based on the correctness of each of the implemented functions.\n",
        "\n",
        "- If all the required functions are implemented correctly, you should expect a UAS in a range of [0.64, 0.67], a LAS in a range of [0.56, 0.59] without changing the parameters of the neural model or the whole training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9PweOgvvjKc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7fabd7f7-b0cd-4bfd-8683-562d52015c02"
      },
      "source": [
        "embeddingsFile = \"glove.6B.50d.txt\"\n",
        "trainFile = \"train.projective.short.conll\"\n",
        "devFile = \"dev.projective.conll\"\n",
        "\n",
        "embeddings, vocab=load_embeddings(embeddingsFile)\n",
        "pos_tag_vocab=get_pos_tag_vocab(trainFile)\n",
        "word_feats, pos_feats, labels = get_oracles(trainFile, vocab, pos_tag_vocab)\n",
        "\n",
        "label_vocab, reverse_labels=get_label_vocab(labels)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word_embedding_dim: 50, vocab size: 400004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lFN4U_1ojKc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "0a90cc2a-4ff6-4ca1-de71-9f8aba305d31"
      },
      "source": [
        "model = train(word_feats, pos_feats, labels, embeddings, vocab, pos_tag_vocab, label_vocab)\n",
        "evaluate(devFile, model, vocab, pos_tag_vocab, reverse_labels)\n",
        "test(model, vocab, pos_tag_vocab, reverse_labels)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss:  2097.9754111766815\n",
            "loss:  1803.350136935711\n",
            "loss:  1734.3115628361702\n",
            "loss:  1701.0472575426102\n",
            "loss:  1673.9843604564667\n",
            "loss:  1657.0750904083252\n",
            "loss:  1640.5775406360626\n",
            "loss:  1628.0692977309227\n",
            "loss:  1618.8904757499695\n",
            "loss:  1612.0517721772194\n",
            "UAS: 0.660, LAS:0.582\n",
            "(1 I) -> (2 bought) nsubj\n",
            "(2 bought) -> (0 <ROOT>) root\n",
            "(3 a) -> (4 book) det\n",
            "(4 book) -> (2 bought) obj\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}